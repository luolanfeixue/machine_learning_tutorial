{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhl/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# from ._conv import register_converters as _register_converters\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "slim = tf.contrib.slim\n",
    "trunc_normal = lambda stddev : tf.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义经常用到的默认参数 如 卷积的集合函数、权重初始化方式、标准化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3_arg_scope(weight_decay=0.0004, stddev=0.1, batch_norm_var_collection='moving_vars'):\n",
    "    batch_norm_params = {\n",
    "        'decay' : 0.9997,\n",
    "        'epsilon' : 0.0001,\n",
    "        'updates_collections' : tf.GraphKeys.UPDATE_OPS,\n",
    "        'variables_collections' : {\n",
    "            'beta' : None,\n",
    "            'gamma' : None,\n",
    "            'moving_mean' : [batch_norm_var_collection],\n",
    "            'moving_variance' : [batch_norm_var_collection],\n",
    "        }\n",
    "    }\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)): \n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d],\n",
    "            weights_initializer=trunc_normal(stddev), \n",
    "            activation_fn=tf.nn.relu,\n",
    "            normalizer_fn=slim.batch_norm, \n",
    "            normalizer_params=batch_norm_params) as sc: \n",
    "            return sc ## 注意这种代码缩进要搞对。不然很容易出错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_v3_base(inputs, scope=None):\n",
    "    end_points = {}\n",
    "    with tf.variable_scope(scope,'InceptionV3',[inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='VALID'):\n",
    "            # 正式定义Inception V3的网络结构。首先是前面的非Inception Module的卷积层\n",
    "            # 使用了slim.arg_scope,使得slim.conv2d,slim.max_pool2d的一些初始化工作都已经做好，一行就可以定义一个卷积。\n",
    "            # 输入：inputs , 299 x 299 x 3\n",
    "            # 第一个参数为输入的tensor，第二个是输出的通道数，卷积核尺寸，步长stride，padding模式\n",
    "            net = slim.conv2d(inputs, 32, [3, 3], stride=2, scope='Conv2d_1a_3x3') # 149 x 149 x 32\n",
    "            net = slim.conv2d(net, 32, [3, 3], scope='Conv2d_2a_3x3') # 147 x 147 x 32\n",
    "            net = slim.conv2d(net, 64, [3, 3], padding='SAME', scope='Conv2d_2b_3x3') # 147 x 147 x 64\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_3a_3x3') # 73 x 73 x 64\n",
    "            # 1x1 的卷积将相关性高、在同一空间位置但不同通道上但特征链接在一起。允许了通道间的信息组合且符合Hebbian原理\n",
    "            net = slim.conv2d(net, 80, [1, 1], scope='Conv2d_3b_1x1')  # 73 x 73 x 80.\n",
    "            net = slim.conv2d(net, 192, [3, 3], scope='Conv2d_4a_3x3') # 71 x 71 x 192.\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_5a_3x3') # 35 x 35 x 192.\n",
    "            \n",
    "            # 上面部分代码一共有5个卷积层，2个池化层，实现了对图片数据的尺寸压缩，并对图片特征进行了抽象\n",
    "            \n",
    "        # Inception blocks\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "            \n",
    "            # 下面是第一个Inception，包含了三个结构类似的Inception Module, Mixed_5b\\Mixed_5c\\Mixed_5d\n",
    "            with tf.variable_scope('Mixed_5b'): # 第一个Inception Module名称。Inception Module有四个分支\n",
    "                with tf.variable_scope('Branch_0'): # 第一个分支64通道的1*1卷积\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope = 'Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'): # 第二个分支48通道1*1卷积，链接一个64通道的5*5卷积\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope = 'Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'): # \n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope = 'Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope = 'Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope = 'Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):# 第四个分支为3*3的平均池化，连接32通道的1*1卷积\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope = 'AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope = 'Conv2_0b_1x1')\n",
    "                # 第四个维度合并，即输出通道上合并。第一个为batch_size, 第二/三为图片尺寸\n",
    "                # 通道合计： 64 + 64 + 96 + 32 = 256, same pading ,所以输出为 35x35x256\n",
    "                # 如果正常从35x35x192变到35x35x256 需要参数量 35*35*192*256 = 60M\n",
    "                # 但是现在参数量为：64 + 48 + 5*5*64 + 64 + 3*3*96*2 + 32 = 3.5k 参数量减少17k倍。\n",
    "                # 同时计算量也减少很多。这就是Inception精妙之处。\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) \n",
    "                \n",
    "            with tf.variable_scope('Mixed_5c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Con2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net,[3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                # 与Mixed_5b 只有branch_3不一样，输出35x35x288\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            with tf.variable_scope('Mixed_5d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Con2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net,[3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                # 与Mixed_5b 只有branch_3不一样，输出35x35x288\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "             # 下面是第二个Inception，包含了5个的Inception Module,Mixed_6a\\Mixed_6b\\Mixed_6c\\Mixed_6d\\Mixed_6e\n",
    "            with tf.variable_scope('Mixed_6a'):# stride=2, 图片被压缩。\n",
    "                with tf.variable_scope('Branch_0'): \n",
    "                    branch_0 = slim.conv2d(net, 384, [3, 3], stride=2, padding='VALID', scope='Conv2d_0a_1x1') \n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, padding='VALID', scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_0a_3x3')\n",
    "                # （35-3）/2 + 1 = 17, 最终 17*17*(384+96+288) = 17*17*768\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "            \n",
    "            with tf.variable_scope('Mixed_6b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_01_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    # 串联 1x7 和 7x1 相当于 一个7x7，但是参数量减少为原来的（7+7）/7*7 = 2/7，同时多来一个非线性变换。\n",
    "                    branch_2 = slim.conv2d(net, 128, [1,1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 与Mixed_6b一样，只是将128 变为160 \n",
    "            with tf.variable_scope('Mixed_6c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_01_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    # 串联 1x7 和 7x1 相当于 一个7x7，但是参数量减少为原来的（7+7）/7*7 = 2/7，同时多来一个非线性变换。\n",
    "                    branch_2 = slim.conv2d(net, 160, [1,1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "            # 与Mixed_6c一样\n",
    "            with tf.variable_scope('Mixed_6d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_01_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    # 串联 1x7 和 7x1 相当于 一个7x7，但是参数量减少为原来的（7+7）/7*7 = 2/7，同时多来一个非线性变换。\n",
    "                    branch_2 = slim.conv2d(net, 160, [1,1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "               # 与Mixed_6c，Mixed_6d一样，最终输出17 x 17 x 768\n",
    "            with tf.variable_scope('Mixed_6e'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_01_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    # 串联 1x7 和 7x1 相当于 一个7x7，但是参数量减少为原来的（7+7）/7*7 = 2/7，同时多来一个非线性变换。\n",
    "                    branch_2 = slim.conv2d(net, 160, [1,1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            # 将Mixed_6e存储，作为 Auxiliary Cliassfier 的辅助分类.。 \n",
    "            end_points['Mixed_6e'] = net\n",
    "            \n",
    "            with tf.variable_scope('Mixed_7a'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope = 'Conv2d_0a_1x1')\n",
    "                    branch_0 = slim.conv2d(branch_0,320, [3, 3], stride=2, padding='VALID', scope='Conv2d_0b_3x3')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 192, [1, 1], scope = 'Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2, padding='VALID', scope='Conv2d_0d_3x3')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_0a_3x3')\n",
    "                # 8x8x1280\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "                \n",
    "            with tf.variable_scope('Mixed_7b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 382, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1_a = slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_1b_1x3')\n",
    "                    branch_1_b = slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_2b_3x1')\n",
    "                    branch_1 = tf.concat([branch_1_a, branch_1_b], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2,384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2_a = slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_1c_1x3')\n",
    "                    branch_2_b = slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_2c_3x1')\n",
    "                    branch_2 = tf.concat([branch_2_a, branch_2_b], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3],scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                # 8x8x2048\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            \n",
    "            with tf.variable_scope('Mixed_7c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 382, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1_a = slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_1b_1x3')\n",
    "                    branch_1_b = slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_2b_3x1')\n",
    "                    branch_1 = tf.concat([branch_1_a, branch_1_b], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2,384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2_a = slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_1c_1x3')\n",
    "                    branch_2_b = slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_2c_3x1')\n",
    "                    branch_2 = tf.concat([branch_2_a, branch_2_b], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3],scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                # 8x8x2048\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "    return net,end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_v3(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.8, \n",
    "                 prediction_fn=slim.softmax, spatial_squeeze=True, reuse=None, scope='InceptionV3'):\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes],reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            print(inputs.shape)\n",
    "            net, end_points = inception_v3_base(inputs, scope=scope)\n",
    "            with slim.arg_scope([slim.conv2d,slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "                aux_logits = end_points['Mixed_6e'] # 17x17x768\n",
    "                with tf.variable_scope('AuxLogits'):\n",
    "                    aux_logits = slim.avg_pool2d(aux_logits,[5, 5], stride=3, padding='VALID', scope='AvgPool_1a_5x5')\n",
    "                    # 5x5x768\n",
    "                    aux_logits = slim.conv2d(aux_logits, 128,[1, 1], scope='Conv2d_1b_1x1') # 5x5x768\n",
    "                    aux_logits = slim.conv2d(aux_logits, 768, [5, 5], weights_initializer=trunc_normal(0.01),\n",
    "                                            padding='VALID', scope='Conv2d_2a_5x5')\n",
    "                    aux_logits = slim.conv2d(aux_logits, num_classes, [1, 1], activation_fn=None,\n",
    "                                            normalizer_fn=None,weights_initializer=trunc_normal(0.001),\n",
    "                                            scope='Conv2d_2b_1x1')\n",
    "                    if spatial_squeeze:\n",
    "#                         aux_logits = tf.square(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "                        aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "                    end_points['AuxLogits'] = aux_logits\n",
    "        with tf.variable_scope('Logits'):\n",
    "            net = slim.avg_pool2d(net, [8, 8], padding='VALID',\n",
    "                                  scope='AvgPool_1a_8x8')\n",
    "            # 1 x 1 x 2048\n",
    "            net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "            end_points['PreLogits'] = net\n",
    "            # 2048\n",
    "            logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, # 输出通道数1000\n",
    "                                 normalizer_fn=None, scope='Conv2d_1c_1x1') # 激活函数和规范化函数设为空\n",
    "            if spatial_squeeze: # tf.squeeze去除输出tensor中维度为1的节点\n",
    "              logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "            # 1000\n",
    "        end_points['Logits'] = logits\n",
    "        end_points['Predictions'] = prediction_fn(logits, scope='Predictions') # Softmax对结果进行分类预测\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_tensorflow_run(session, target, info_string):\n",
    "    num_steps_burn_in = 10\n",
    "    total_duration = 0.0\n",
    "    total_duration_squared = 0.0\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time =time.time()\n",
    "        _ = session.run(target)\n",
    "        duration = time.time() - start_time;\n",
    "        if i >= num_steps_burn_in:\n",
    "            if not i % 10 :\n",
    "                print('%s: step%d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n",
    "            total_duration += duration;\n",
    "            total_duration_squared += duration*duration\n",
    "        mn = total_duration / num_batches\n",
    "        vr = total_duration_squared / num_batches - mn* mn\n",
    "        sd = math.sqrt(vr)\n",
    "        print('$%s : %s across %d steps, %.3f +/- %.3f sec /batch' % (datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 299, 299, 3)\n",
      "$2018-11-11 14:49:23.619205 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:23.692213 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:23.766262 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:23.840284 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:23.913778 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:23.987508 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:24.061269 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:24.134657 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:24.208033 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "$2018-11-11 14:49:24.281725 : Forward across 100 steps, 0.000 +/- 0.000 sec /batch\n",
      "2018-11-11 14:49:24.355039: step0, duration = 0.073\n",
      "$2018-11-11 14:49:24.355148 : Forward across 100 steps, 0.001 +/- 0.007 sec /batch\n",
      "$2018-11-11 14:49:24.429185 : Forward across 100 steps, 0.001 +/- 0.010 sec /batch\n",
      "$2018-11-11 14:49:24.502717 : Forward across 100 steps, 0.002 +/- 0.013 sec /batch\n",
      "$2018-11-11 14:49:24.576115 : Forward across 100 steps, 0.003 +/- 0.014 sec /batch\n",
      "$2018-11-11 14:49:24.649201 : Forward across 100 steps, 0.004 +/- 0.016 sec /batch\n",
      "$2018-11-11 14:49:24.722960 : Forward across 100 steps, 0.004 +/- 0.017 sec /batch\n",
      "$2018-11-11 14:49:24.796492 : Forward across 100 steps, 0.005 +/- 0.019 sec /batch\n",
      "$2018-11-11 14:49:24.869938 : Forward across 100 steps, 0.006 +/- 0.020 sec /batch\n",
      "$2018-11-11 14:49:24.943830 : Forward across 100 steps, 0.007 +/- 0.021 sec /batch\n",
      "$2018-11-11 14:49:25.017448 : Forward across 100 steps, 0.007 +/- 0.022 sec /batch\n",
      "2018-11-11 14:49:25.091007: step10, duration = 0.073\n",
      "$2018-11-11 14:49:25.091098 : Forward across 100 steps, 0.008 +/- 0.023 sec /batch\n",
      "$2018-11-11 14:49:25.164474 : Forward across 100 steps, 0.009 +/- 0.024 sec /batch\n",
      "$2018-11-11 14:49:25.238010 : Forward across 100 steps, 0.010 +/- 0.025 sec /batch\n",
      "$2018-11-11 14:49:25.311621 : Forward across 100 steps, 0.010 +/- 0.025 sec /batch\n",
      "$2018-11-11 14:49:25.385124 : Forward across 100 steps, 0.011 +/- 0.026 sec /batch\n",
      "$2018-11-11 14:49:25.458780 : Forward across 100 steps, 0.012 +/- 0.027 sec /batch\n",
      "$2018-11-11 14:49:25.532477 : Forward across 100 steps, 0.012 +/- 0.028 sec /batch\n",
      "$2018-11-11 14:49:25.606662 : Forward across 100 steps, 0.013 +/- 0.028 sec /batch\n",
      "$2018-11-11 14:49:25.680171 : Forward across 100 steps, 0.014 +/- 0.029 sec /batch\n",
      "$2018-11-11 14:49:25.753807 : Forward across 100 steps, 0.015 +/- 0.029 sec /batch\n",
      "2018-11-11 14:49:25.826821: step20, duration = 0.073\n",
      "$2018-11-11 14:49:25.826868 : Forward across 100 steps, 0.015 +/- 0.030 sec /batch\n",
      "$2018-11-11 14:49:25.900191 : Forward across 100 steps, 0.016 +/- 0.030 sec /batch\n",
      "$2018-11-11 14:49:25.973274 : Forward across 100 steps, 0.017 +/- 0.031 sec /batch\n",
      "$2018-11-11 14:49:26.046600 : Forward across 100 steps, 0.018 +/- 0.031 sec /batch\n",
      "$2018-11-11 14:49:26.120075 : Forward across 100 steps, 0.018 +/- 0.032 sec /batch\n",
      "$2018-11-11 14:49:26.193816 : Forward across 100 steps, 0.019 +/- 0.032 sec /batch\n",
      "$2018-11-11 14:49:26.267344 : Forward across 100 steps, 0.020 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:26.340246 : Forward across 100 steps, 0.021 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:26.413911 : Forward across 100 steps, 0.021 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:26.487592 : Forward across 100 steps, 0.022 +/- 0.034 sec /batch\n",
      "2018-11-11 14:49:26.560185: step30, duration = 0.072\n",
      "$2018-11-11 14:49:26.560288 : Forward across 100 steps, 0.023 +/- 0.034 sec /batch\n",
      "$2018-11-11 14:49:26.634611 : Forward across 100 steps, 0.023 +/- 0.034 sec /batch\n",
      "$2018-11-11 14:49:26.708054 : Forward across 100 steps, 0.024 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:26.781372 : Forward across 100 steps, 0.025 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:26.854597 : Forward across 100 steps, 0.026 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:26.927688 : Forward across 100 steps, 0.026 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:27.000745 : Forward across 100 steps, 0.027 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:27.075085 : Forward across 100 steps, 0.028 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.149157 : Forward across 100 steps, 0.029 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.222780 : Forward across 100 steps, 0.029 +/- 0.036 sec /batch\n",
      "2018-11-11 14:49:27.296776: step40, duration = 0.074\n",
      "$2018-11-11 14:49:27.296911 : Forward across 100 steps, 0.030 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.370419 : Forward across 100 steps, 0.031 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.443911 : Forward across 100 steps, 0.032 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.517580 : Forward across 100 steps, 0.032 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:27.591396 : Forward across 100 steps, 0.033 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:27.664990 : Forward across 100 steps, 0.034 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:27.738573 : Forward across 100 steps, 0.035 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:27.812123 : Forward across 100 steps, 0.035 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:27.885888 : Forward across 100 steps, 0.036 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:27.959685 : Forward across 100 steps, 0.037 +/- 0.037 sec /batch\n",
      "2018-11-11 14:49:28.033328: step50, duration = 0.073\n",
      "$2018-11-11 14:49:28.033577 : Forward across 100 steps, 0.037 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:28.107323 : Forward across 100 steps, 0.038 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:28.180512 : Forward across 100 steps, 0.039 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:28.254807 : Forward across 100 steps, 0.040 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:28.328762 : Forward across 100 steps, 0.040 +/- 0.037 sec /batch\n",
      "$2018-11-11 14:49:28.402018 : Forward across 100 steps, 0.041 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.476230 : Forward across 100 steps, 0.042 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.549889 : Forward across 100 steps, 0.043 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.623745 : Forward across 100 steps, 0.043 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.697089 : Forward across 100 steps, 0.044 +/- 0.036 sec /batch\n",
      "2018-11-11 14:49:28.771026: step60, duration = 0.074\n",
      "$2018-11-11 14:49:28.771130 : Forward across 100 steps, 0.045 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.844661 : Forward across 100 steps, 0.046 +/- 0.036 sec /batch\n",
      "$2018-11-11 14:49:28.918700 : Forward across 100 steps, 0.046 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:28.992742 : Forward across 100 steps, 0.047 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:29.067021 : Forward across 100 steps, 0.048 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:29.141121 : Forward across 100 steps, 0.049 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:29.214685 : Forward across 100 steps, 0.049 +/- 0.035 sec /batch\n",
      "$2018-11-11 14:49:29.288744 : Forward across 100 steps, 0.050 +/- 0.034 sec /batch\n",
      "$2018-11-11 14:49:29.362327 : Forward across 100 steps, 0.051 +/- 0.034 sec /batch\n",
      "$2018-11-11 14:49:29.436176 : Forward across 100 steps, 0.051 +/- 0.034 sec /batch\n",
      "2018-11-11 14:49:29.509133: step70, duration = 0.073\n",
      "$2018-11-11 14:49:29.509195 : Forward across 100 steps, 0.052 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:29.583408 : Forward across 100 steps, 0.053 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:29.657624 : Forward across 100 steps, 0.054 +/- 0.033 sec /batch\n",
      "$2018-11-11 14:49:29.731289 : Forward across 100 steps, 0.054 +/- 0.032 sec /batch\n",
      "$2018-11-11 14:49:29.805917 : Forward across 100 steps, 0.055 +/- 0.032 sec /batch\n",
      "$2018-11-11 14:49:29.880785 : Forward across 100 steps, 0.056 +/- 0.031 sec /batch\n",
      "$2018-11-11 14:49:29.954424 : Forward across 100 steps, 0.057 +/- 0.031 sec /batch\n",
      "$2018-11-11 14:49:30.028132 : Forward across 100 steps, 0.057 +/- 0.030 sec /batch\n",
      "$2018-11-11 14:49:30.102173 : Forward across 100 steps, 0.058 +/- 0.030 sec /batch\n",
      "$2018-11-11 14:49:30.175582 : Forward across 100 steps, 0.059 +/- 0.029 sec /batch\n",
      "2018-11-11 14:49:30.249572: step80, duration = 0.074\n",
      "$2018-11-11 14:49:30.249894 : Forward across 100 steps, 0.060 +/- 0.029 sec /batch\n",
      "$2018-11-11 14:49:30.324334 : Forward across 100 steps, 0.060 +/- 0.028 sec /batch\n",
      "$2018-11-11 14:49:30.397647 : Forward across 100 steps, 0.061 +/- 0.028 sec /batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$2018-11-11 14:49:30.471662 : Forward across 100 steps, 0.062 +/- 0.027 sec /batch\n",
      "$2018-11-11 14:49:30.545396 : Forward across 100 steps, 0.063 +/- 0.026 sec /batch\n",
      "$2018-11-11 14:49:30.619198 : Forward across 100 steps, 0.063 +/- 0.026 sec /batch\n",
      "$2018-11-11 14:49:30.692427 : Forward across 100 steps, 0.064 +/- 0.025 sec /batch\n",
      "$2018-11-11 14:49:30.766545 : Forward across 100 steps, 0.065 +/- 0.024 sec /batch\n",
      "$2018-11-11 14:49:30.840636 : Forward across 100 steps, 0.065 +/- 0.023 sec /batch\n",
      "$2018-11-11 14:49:30.913694 : Forward across 100 steps, 0.066 +/- 0.022 sec /batch\n",
      "2018-11-11 14:49:30.988071: step90, duration = 0.074\n",
      "$2018-11-11 14:49:30.988226 : Forward across 100 steps, 0.067 +/- 0.021 sec /batch\n",
      "$2018-11-11 14:49:31.061697 : Forward across 100 steps, 0.068 +/- 0.020 sec /batch\n",
      "$2018-11-11 14:49:31.135573 : Forward across 100 steps, 0.068 +/- 0.019 sec /batch\n",
      "$2018-11-11 14:49:31.209282 : Forward across 100 steps, 0.069 +/- 0.017 sec /batch\n",
      "$2018-11-11 14:49:31.282823 : Forward across 100 steps, 0.070 +/- 0.016 sec /batch\n",
      "$2018-11-11 14:49:31.355798 : Forward across 100 steps, 0.071 +/- 0.014 sec /batch\n",
      "$2018-11-11 14:49:31.429900 : Forward across 100 steps, 0.071 +/- 0.013 sec /batch\n",
      "$2018-11-11 14:49:31.503871 : Forward across 100 steps, 0.072 +/- 0.010 sec /batch\n",
      "$2018-11-11 14:49:31.577399 : Forward across 100 steps, 0.073 +/- 0.007 sec /batch\n",
      "$2018-11-11 14:49:31.651657 : Forward across 100 steps, 0.074 +/- 0.000 sec /batch\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "height, width = 299,299\n",
    "inputs = tf.random_uniform((batch_size,height, width, 3))\n",
    "with slim.arg_scope(inception_v3_arg_scope()):\n",
    "    logits,end_points = inception_v3(inputs, is_training=True)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "num_batches = 100\n",
    "time_tensorflow_run(sess,logits, 'Forward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
