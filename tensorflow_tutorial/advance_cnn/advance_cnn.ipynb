{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhl/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cifar10, cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "定义batch_size 迭代次数 和cifar-10的数据默认路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_steps = 3000\n",
    "batch_size = 128\n",
    "data_dir = '/home/hhl/machine_learning_tutorial/tensorflow_tutorial/advance_cnn/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe_download_and_extract() 报错\n",
    "UnrecognizedFlagError: Unknown command line flag 'f'\n",
    "\n",
    "解决方案 tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir = data_dir,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_test,labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* var 是正常的参数，l2_loss 是将var的正则未来要放到loss中，w1是该正则的系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_with_weigth_loss(shape, stddev, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev = stddev))\n",
    "    if w1 is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), w1, name = 'weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24,24, 3])\n",
    "label_holder = tf.placeholder(tf.float32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 第一层卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight1 = variable_with_weigth_loss(shape = [5, 5, 3, 64], stddev = 0.05, w1 = 0.0)\n",
    "kernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding = 'SAME')\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape = [64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize = [1, 3, 3, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias = 1.0, alpha = 0.001 / 9.0, beta = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第二层卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight2 = variable_with_weigth_loss(shape = [5, 5, 64, 64], stddev = 0.05, w1 = 0.0)\n",
    "kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding = 'SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias = 1.0, alpha=0.001/9.9,beta = 0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize = [1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 全链接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weigth_loss(shape=[dim,384], stddev=0.04, w1=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 第二全链接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight4 = variable_with_weigth_loss(shape = [384,192], stddev = 0.04, w1 = 0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape = [192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight5 = variable_with_weigth_loss(shape = [192, 10], stddev=1/192.0, w1 = 0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5),bias5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 以上即使inference 的部分，即正向传播的部分。\n",
    " \n",
    " 下面开始计算loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits,labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, \n",
    "                                                                   labels = labels, \n",
    "                                                                   name = 'cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name = 'cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name = 'total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = loss(logits,label_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_holder2 = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k_op = tf.nn.in_top_k(logits,label_holder2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 139887193868032)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139887185475328)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139887177082624)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139887168689920)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886682175232)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886673782528)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886665389824)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886656997120)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886640211712)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886631819008)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886078195456)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886069802752)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886061410048)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886053017344)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886044624640)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886036231936)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 139886648604416)>,\n",
       " <Thread(QueueRunnerThread-input/input_producer-input/input_producer/input_producer_EnqueueMany, started daemon 139886027839232)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885541324544)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885532931840)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885524539136)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885507753728)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885516146432)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885499361024)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885490968320)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139885004453632)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884996060928)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884979275520)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884987668224)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884970882816)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884962490112)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884954097408)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884467582720)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 139884459190016)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 4.68 ( 12.5 example/sec; 10.206 sec/batch)\n",
      "step 10, loss = 3.68 ( 1760.4 example/sec; 0.073 sec/batch)\n",
      "step 20, loss = 3.05 ( 1791.1 example/sec; 0.071 sec/batch)\n",
      "step 30, loss = 2.70 ( 1949.7 example/sec; 0.066 sec/batch)\n",
      "step 40, loss = 2.38 ( 1866.6 example/sec; 0.069 sec/batch)\n",
      "step 50, loss = 2.35 ( 1670.9 example/sec; 0.077 sec/batch)\n",
      "step 60, loss = 2.12 ( 2090.5 example/sec; 0.061 sec/batch)\n",
      "step 70, loss = 2.17 ( 1826.4 example/sec; 0.070 sec/batch)\n",
      "step 80, loss = 1.92 ( 1507.4 example/sec; 0.085 sec/batch)\n",
      "step 90, loss = 1.95 ( 1598.1 example/sec; 0.080 sec/batch)\n",
      "step 100, loss = 1.97 ( 2181.3 example/sec; 0.059 sec/batch)\n",
      "step 110, loss = 1.90 ( 1668.2 example/sec; 0.077 sec/batch)\n",
      "step 120, loss = 1.95 ( 1815.5 example/sec; 0.071 sec/batch)\n",
      "step 130, loss = 1.84 ( 1838.5 example/sec; 0.070 sec/batch)\n",
      "step 140, loss = 1.83 ( 1763.5 example/sec; 0.073 sec/batch)\n",
      "step 150, loss = 1.94 ( 1770.1 example/sec; 0.072 sec/batch)\n",
      "step 160, loss = 1.79 ( 1664.6 example/sec; 0.077 sec/batch)\n",
      "step 170, loss = 1.70 ( 1720.3 example/sec; 0.074 sec/batch)\n",
      "step 180, loss = 1.55 ( 1743.1 example/sec; 0.073 sec/batch)\n",
      "step 190, loss = 1.77 ( 1877.6 example/sec; 0.068 sec/batch)\n",
      "step 200, loss = 1.72 ( 1580.2 example/sec; 0.081 sec/batch)\n",
      "step 210, loss = 1.80 ( 1889.1 example/sec; 0.068 sec/batch)\n",
      "step 220, loss = 1.84 ( 1710.2 example/sec; 0.075 sec/batch)\n",
      "step 230, loss = 1.79 ( 1821.6 example/sec; 0.070 sec/batch)\n",
      "step 240, loss = 1.64 ( 1678.2 example/sec; 0.076 sec/batch)\n",
      "step 250, loss = 1.54 ( 1567.2 example/sec; 0.082 sec/batch)\n",
      "step 260, loss = 1.57 ( 1703.8 example/sec; 0.075 sec/batch)\n",
      "step 270, loss = 1.69 ( 1890.3 example/sec; 0.068 sec/batch)\n",
      "step 280, loss = 1.67 ( 1760.2 example/sec; 0.073 sec/batch)\n",
      "step 290, loss = 1.63 ( 1839.5 example/sec; 0.070 sec/batch)\n",
      "step 300, loss = 1.68 ( 1651.7 example/sec; 0.077 sec/batch)\n",
      "step 310, loss = 1.62 ( 1876.3 example/sec; 0.068 sec/batch)\n",
      "step 320, loss = 1.73 ( 1604.5 example/sec; 0.080 sec/batch)\n",
      "step 330, loss = 1.42 ( 1679.5 example/sec; 0.076 sec/batch)\n",
      "step 340, loss = 1.46 ( 1606.7 example/sec; 0.080 sec/batch)\n",
      "step 350, loss = 1.73 ( 1623.3 example/sec; 0.079 sec/batch)\n",
      "step 360, loss = 1.59 ( 1681.8 example/sec; 0.076 sec/batch)\n",
      "step 370, loss = 1.70 ( 1763.6 example/sec; 0.073 sec/batch)\n",
      "step 380, loss = 1.51 ( 1890.9 example/sec; 0.068 sec/batch)\n",
      "step 390, loss = 1.45 ( 1731.5 example/sec; 0.074 sec/batch)\n",
      "step 400, loss = 1.47 ( 1919.2 example/sec; 0.067 sec/batch)\n",
      "step 410, loss = 1.69 ( 1581.7 example/sec; 0.081 sec/batch)\n",
      "step 420, loss = 1.60 ( 1459.8 example/sec; 0.088 sec/batch)\n",
      "step 430, loss = 1.77 ( 1763.2 example/sec; 0.073 sec/batch)\n",
      "step 440, loss = 1.49 ( 1584.7 example/sec; 0.081 sec/batch)\n",
      "step 450, loss = 1.40 ( 1820.4 example/sec; 0.070 sec/batch)\n",
      "step 460, loss = 1.44 ( 1890.2 example/sec; 0.068 sec/batch)\n",
      "step 470, loss = 1.56 ( 1750.9 example/sec; 0.073 sec/batch)\n",
      "step 480, loss = 1.57 ( 1703.2 example/sec; 0.075 sec/batch)\n",
      "step 490, loss = 1.48 ( 2102.1 example/sec; 0.061 sec/batch)\n",
      "step 500, loss = 1.48 ( 1797.5 example/sec; 0.071 sec/batch)\n",
      "step 510, loss = 1.26 ( 1641.4 example/sec; 0.078 sec/batch)\n",
      "step 520, loss = 1.48 ( 1619.6 example/sec; 0.079 sec/batch)\n",
      "step 530, loss = 1.55 ( 1875.8 example/sec; 0.068 sec/batch)\n",
      "step 540, loss = 1.53 ( 1785.3 example/sec; 0.072 sec/batch)\n",
      "step 550, loss = 1.29 ( 1829.7 example/sec; 0.070 sec/batch)\n",
      "step 560, loss = 1.54 ( 1905.2 example/sec; 0.067 sec/batch)\n",
      "step 570, loss = 1.40 ( 1832.7 example/sec; 0.070 sec/batch)\n",
      "step 580, loss = 1.36 ( 1836.9 example/sec; 0.070 sec/batch)\n",
      "step 590, loss = 1.54 ( 1704.4 example/sec; 0.075 sec/batch)\n",
      "step 600, loss = 1.41 ( 1491.1 example/sec; 0.086 sec/batch)\n",
      "step 610, loss = 1.35 ( 1696.4 example/sec; 0.075 sec/batch)\n",
      "step 620, loss = 1.29 ( 1667.0 example/sec; 0.077 sec/batch)\n",
      "step 630, loss = 1.52 ( 1815.1 example/sec; 0.071 sec/batch)\n",
      "step 640, loss = 1.33 ( 1783.4 example/sec; 0.072 sec/batch)\n",
      "step 650, loss = 1.39 ( 1762.2 example/sec; 0.073 sec/batch)\n",
      "step 660, loss = 1.46 ( 1779.1 example/sec; 0.072 sec/batch)\n",
      "step 670, loss = 1.23 ( 1613.1 example/sec; 0.079 sec/batch)\n",
      "step 680, loss = 1.37 ( 1826.1 example/sec; 0.070 sec/batch)\n",
      "step 690, loss = 1.29 ( 1832.2 example/sec; 0.070 sec/batch)\n",
      "step 700, loss = 1.39 ( 1521.7 example/sec; 0.084 sec/batch)\n",
      "step 710, loss = 1.43 ( 1660.6 example/sec; 0.077 sec/batch)\n",
      "step 720, loss = 1.31 ( 1603.1 example/sec; 0.080 sec/batch)\n",
      "step 730, loss = 1.39 ( 1798.5 example/sec; 0.071 sec/batch)\n",
      "step 740, loss = 1.41 ( 1598.7 example/sec; 0.080 sec/batch)\n",
      "step 750, loss = 1.32 ( 1651.7 example/sec; 0.077 sec/batch)\n",
      "step 760, loss = 1.35 ( 1644.8 example/sec; 0.078 sec/batch)\n",
      "step 770, loss = 1.22 ( 1561.2 example/sec; 0.082 sec/batch)\n",
      "step 780, loss = 1.31 ( 1882.1 example/sec; 0.068 sec/batch)\n",
      "step 790, loss = 1.21 ( 1803.1 example/sec; 0.071 sec/batch)\n",
      "step 800, loss = 1.35 ( 1695.2 example/sec; 0.076 sec/batch)\n",
      "step 810, loss = 1.28 ( 1931.4 example/sec; 0.066 sec/batch)\n",
      "step 820, loss = 1.46 ( 1486.8 example/sec; 0.086 sec/batch)\n",
      "step 830, loss = 1.47 ( 1784.4 example/sec; 0.072 sec/batch)\n",
      "step 840, loss = 1.26 ( 1746.2 example/sec; 0.073 sec/batch)\n",
      "step 850, loss = 1.17 ( 1698.0 example/sec; 0.075 sec/batch)\n",
      "step 860, loss = 1.20 ( 1627.1 example/sec; 0.079 sec/batch)\n",
      "step 870, loss = 1.28 ( 1560.1 example/sec; 0.082 sec/batch)\n",
      "step 880, loss = 1.10 ( 1714.4 example/sec; 0.075 sec/batch)\n",
      "step 890, loss = 1.33 ( 1720.5 example/sec; 0.074 sec/batch)\n",
      "step 900, loss = 1.18 ( 1824.9 example/sec; 0.070 sec/batch)\n",
      "step 910, loss = 1.39 ( 1975.1 example/sec; 0.065 sec/batch)\n",
      "step 920, loss = 1.35 ( 1565.7 example/sec; 0.082 sec/batch)\n",
      "step 930, loss = 1.34 ( 2004.0 example/sec; 0.064 sec/batch)\n",
      "step 940, loss = 1.28 ( 1797.9 example/sec; 0.071 sec/batch)\n",
      "step 950, loss = 1.40 ( 1666.2 example/sec; 0.077 sec/batch)\n",
      "step 960, loss = 1.24 ( 1812.2 example/sec; 0.071 sec/batch)\n",
      "step 970, loss = 1.05 ( 1677.7 example/sec; 0.076 sec/batch)\n",
      "step 980, loss = 1.23 ( 1653.6 example/sec; 0.077 sec/batch)\n",
      "step 990, loss = 1.36 ( 1757.5 example/sec; 0.073 sec/batch)\n",
      "step 1000, loss = 1.34 ( 1893.9 example/sec; 0.068 sec/batch)\n",
      "step 1010, loss = 1.23 ( 1837.5 example/sec; 0.070 sec/batch)\n",
      "step 1020, loss = 1.25 ( 1638.5 example/sec; 0.078 sec/batch)\n",
      "step 1030, loss = 1.26 ( 1850.2 example/sec; 0.069 sec/batch)\n",
      "step 1040, loss = 1.32 ( 1910.2 example/sec; 0.067 sec/batch)\n",
      "step 1050, loss = 1.29 ( 1796.1 example/sec; 0.071 sec/batch)\n",
      "step 1060, loss = 1.21 ( 1846.8 example/sec; 0.069 sec/batch)\n",
      "step 1070, loss = 1.28 ( 1698.8 example/sec; 0.075 sec/batch)\n",
      "step 1080, loss = 1.27 ( 1804.1 example/sec; 0.071 sec/batch)\n",
      "step 1090, loss = 1.30 ( 1901.4 example/sec; 0.067 sec/batch)\n",
      "step 1100, loss = 1.34 ( 1526.3 example/sec; 0.084 sec/batch)\n",
      "step 1110, loss = 1.31 ( 1744.9 example/sec; 0.073 sec/batch)\n",
      "step 1120, loss = 1.39 ( 1746.4 example/sec; 0.073 sec/batch)\n",
      "step 1130, loss = 1.15 ( 1871.2 example/sec; 0.068 sec/batch)\n",
      "step 1140, loss = 1.40 ( 2005.3 example/sec; 0.064 sec/batch)\n",
      "step 1150, loss = 1.26 ( 1766.4 example/sec; 0.072 sec/batch)\n",
      "step 1160, loss = 1.29 ( 1635.3 example/sec; 0.078 sec/batch)\n",
      "step 1170, loss = 1.29 ( 1763.3 example/sec; 0.073 sec/batch)\n",
      "step 1180, loss = 1.25 ( 1834.8 example/sec; 0.070 sec/batch)\n",
      "step 1190, loss = 1.42 ( 1603.3 example/sec; 0.080 sec/batch)\n",
      "step 1200, loss = 1.26 ( 1646.2 example/sec; 0.078 sec/batch)\n",
      "step 1210, loss = 1.32 ( 1654.2 example/sec; 0.077 sec/batch)\n",
      "step 1220, loss = 1.15 ( 1768.0 example/sec; 0.072 sec/batch)\n",
      "step 1230, loss = 1.07 ( 1554.0 example/sec; 0.082 sec/batch)\n",
      "step 1240, loss = 1.20 ( 1628.2 example/sec; 0.079 sec/batch)\n",
      "step 1250, loss = 1.30 ( 1357.8 example/sec; 0.094 sec/batch)\n",
      "step 1260, loss = 1.14 ( 1708.1 example/sec; 0.075 sec/batch)\n",
      "step 1270, loss = 1.12 ( 1707.2 example/sec; 0.075 sec/batch)\n",
      "step 1280, loss = 1.19 ( 1933.0 example/sec; 0.066 sec/batch)\n",
      "step 1290, loss = 1.21 ( 1914.5 example/sec; 0.067 sec/batch)\n",
      "step 1300, loss = 1.08 ( 1533.5 example/sec; 0.083 sec/batch)\n",
      "step 1310, loss = 1.09 ( 1689.2 example/sec; 0.076 sec/batch)\n",
      "step 1320, loss = 1.14 ( 2022.3 example/sec; 0.063 sec/batch)\n",
      "step 1330, loss = 1.15 ( 1583.0 example/sec; 0.081 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1340, loss = 1.28 ( 1725.6 example/sec; 0.074 sec/batch)\n",
      "step 1350, loss = 1.29 ( 1695.1 example/sec; 0.076 sec/batch)\n",
      "step 1360, loss = 1.22 ( 1784.6 example/sec; 0.072 sec/batch)\n",
      "step 1370, loss = 1.41 ( 1808.1 example/sec; 0.071 sec/batch)\n",
      "step 1380, loss = 1.17 ( 1915.3 example/sec; 0.067 sec/batch)\n",
      "step 1390, loss = 1.20 ( 1755.8 example/sec; 0.073 sec/batch)\n",
      "step 1400, loss = 1.28 ( 2043.6 example/sec; 0.063 sec/batch)\n",
      "step 1410, loss = 1.11 ( 1811.4 example/sec; 0.071 sec/batch)\n",
      "step 1420, loss = 1.02 ( 1704.6 example/sec; 0.075 sec/batch)\n",
      "step 1430, loss = 1.04 ( 1871.2 example/sec; 0.068 sec/batch)\n",
      "step 1440, loss = 1.14 ( 1693.1 example/sec; 0.076 sec/batch)\n",
      "step 1450, loss = 1.19 ( 1932.2 example/sec; 0.066 sec/batch)\n",
      "step 1460, loss = 1.14 ( 1635.7 example/sec; 0.078 sec/batch)\n",
      "step 1470, loss = 1.08 ( 1604.2 example/sec; 0.080 sec/batch)\n",
      "step 1480, loss = 1.38 ( 1858.8 example/sec; 0.069 sec/batch)\n",
      "step 1490, loss = 1.21 ( 1925.8 example/sec; 0.066 sec/batch)\n",
      "step 1500, loss = 1.27 ( 1924.9 example/sec; 0.066 sec/batch)\n",
      "step 1510, loss = 1.31 ( 1790.8 example/sec; 0.071 sec/batch)\n",
      "step 1520, loss = 1.26 ( 1939.4 example/sec; 0.066 sec/batch)\n",
      "step 1530, loss = 1.09 ( 1879.4 example/sec; 0.068 sec/batch)\n",
      "step 1540, loss = 1.25 ( 1670.0 example/sec; 0.077 sec/batch)\n",
      "step 1550, loss = 1.23 ( 1781.1 example/sec; 0.072 sec/batch)\n",
      "step 1560, loss = 1.25 ( 1641.6 example/sec; 0.078 sec/batch)\n",
      "step 1570, loss = 1.15 ( 1775.8 example/sec; 0.072 sec/batch)\n",
      "step 1580, loss = 1.01 ( 1686.3 example/sec; 0.076 sec/batch)\n",
      "step 1590, loss = 1.08 ( 1747.5 example/sec; 0.073 sec/batch)\n",
      "step 1600, loss = 1.16 ( 1544.9 example/sec; 0.083 sec/batch)\n",
      "step 1610, loss = 1.30 ( 1855.1 example/sec; 0.069 sec/batch)\n",
      "step 1620, loss = 1.07 ( 1659.7 example/sec; 0.077 sec/batch)\n",
      "step 1630, loss = 1.10 ( 1608.8 example/sec; 0.080 sec/batch)\n",
      "step 1640, loss = 1.44 ( 1634.4 example/sec; 0.078 sec/batch)\n",
      "step 1650, loss = 1.23 ( 1820.8 example/sec; 0.070 sec/batch)\n",
      "step 1660, loss = 1.13 ( 1680.0 example/sec; 0.076 sec/batch)\n",
      "step 1670, loss = 1.26 ( 1689.8 example/sec; 0.076 sec/batch)\n",
      "step 1680, loss = 1.16 ( 1869.0 example/sec; 0.068 sec/batch)\n",
      "step 1690, loss = 1.19 ( 1895.9 example/sec; 0.068 sec/batch)\n",
      "step 1700, loss = 1.41 ( 1586.4 example/sec; 0.081 sec/batch)\n",
      "step 1710, loss = 1.16 ( 1733.0 example/sec; 0.074 sec/batch)\n",
      "step 1720, loss = 1.23 ( 1601.8 example/sec; 0.080 sec/batch)\n",
      "step 1730, loss = 1.10 ( 1695.9 example/sec; 0.075 sec/batch)\n",
      "step 1740, loss = 1.19 ( 1608.0 example/sec; 0.080 sec/batch)\n",
      "step 1750, loss = 1.32 ( 1722.0 example/sec; 0.074 sec/batch)\n",
      "step 1760, loss = 1.21 ( 1803.7 example/sec; 0.071 sec/batch)\n",
      "step 1770, loss = 1.43 ( 1826.5 example/sec; 0.070 sec/batch)\n",
      "step 1780, loss = 1.15 ( 1634.6 example/sec; 0.078 sec/batch)\n",
      "step 1790, loss = 1.15 ( 1741.4 example/sec; 0.074 sec/batch)\n",
      "step 1800, loss = 1.19 ( 1674.6 example/sec; 0.076 sec/batch)\n",
      "step 1810, loss = 1.22 ( 1832.5 example/sec; 0.070 sec/batch)\n",
      "step 1820, loss = 1.10 ( 1733.1 example/sec; 0.074 sec/batch)\n",
      "step 1830, loss = 1.13 ( 1682.5 example/sec; 0.076 sec/batch)\n",
      "step 1840, loss = 1.27 ( 1718.8 example/sec; 0.074 sec/batch)\n",
      "step 1850, loss = 1.16 ( 1851.5 example/sec; 0.069 sec/batch)\n",
      "step 1860, loss = 1.22 ( 1627.3 example/sec; 0.079 sec/batch)\n",
      "step 1870, loss = 1.26 ( 1507.1 example/sec; 0.085 sec/batch)\n",
      "step 1880, loss = 1.08 ( 1648.9 example/sec; 0.078 sec/batch)\n",
      "step 1890, loss = 1.09 ( 1830.0 example/sec; 0.070 sec/batch)\n",
      "step 1900, loss = 1.06 ( 1638.9 example/sec; 0.078 sec/batch)\n",
      "step 1910, loss = 0.96 ( 1402.5 example/sec; 0.091 sec/batch)\n",
      "step 1920, loss = 1.25 ( 1902.4 example/sec; 0.067 sec/batch)\n",
      "step 1930, loss = 1.18 ( 1750.7 example/sec; 0.073 sec/batch)\n",
      "step 1940, loss = 1.35 ( 1695.5 example/sec; 0.075 sec/batch)\n",
      "step 1950, loss = 1.21 ( 1684.3 example/sec; 0.076 sec/batch)\n",
      "step 1960, loss = 1.19 ( 1662.2 example/sec; 0.077 sec/batch)\n",
      "step 1970, loss = 1.03 ( 1752.5 example/sec; 0.073 sec/batch)\n",
      "step 1980, loss = 1.26 ( 1686.6 example/sec; 0.076 sec/batch)\n",
      "step 1990, loss = 1.20 ( 1586.5 example/sec; 0.081 sec/batch)\n",
      "step 2000, loss = 1.33 ( 1683.5 example/sec; 0.076 sec/batch)\n",
      "step 2010, loss = 1.14 ( 1647.1 example/sec; 0.078 sec/batch)\n",
      "step 2020, loss = 1.00 ( 1899.3 example/sec; 0.067 sec/batch)\n",
      "step 2030, loss = 1.14 ( 1810.7 example/sec; 0.071 sec/batch)\n",
      "step 2040, loss = 1.10 ( 1504.4 example/sec; 0.085 sec/batch)\n",
      "step 2050, loss = 1.11 ( 1656.0 example/sec; 0.077 sec/batch)\n",
      "step 2060, loss = 1.17 ( 1742.4 example/sec; 0.073 sec/batch)\n",
      "step 2070, loss = 1.34 ( 1556.4 example/sec; 0.082 sec/batch)\n",
      "step 2080, loss = 1.33 ( 1784.3 example/sec; 0.072 sec/batch)\n",
      "step 2090, loss = 1.23 ( 1651.3 example/sec; 0.078 sec/batch)\n",
      "step 2100, loss = 1.04 ( 1668.9 example/sec; 0.077 sec/batch)\n",
      "step 2110, loss = 0.96 ( 1813.2 example/sec; 0.071 sec/batch)\n",
      "step 2120, loss = 1.22 ( 1620.8 example/sec; 0.079 sec/batch)\n",
      "step 2130, loss = 1.18 ( 1693.5 example/sec; 0.076 sec/batch)\n",
      "step 2140, loss = 1.21 ( 1763.8 example/sec; 0.073 sec/batch)\n",
      "step 2150, loss = 1.25 ( 1759.7 example/sec; 0.073 sec/batch)\n",
      "step 2160, loss = 1.04 ( 1890.7 example/sec; 0.068 sec/batch)\n",
      "step 2170, loss = 0.89 ( 1686.2 example/sec; 0.076 sec/batch)\n",
      "step 2180, loss = 1.32 ( 1996.8 example/sec; 0.064 sec/batch)\n",
      "step 2190, loss = 1.29 ( 1722.2 example/sec; 0.074 sec/batch)\n",
      "step 2200, loss = 0.96 ( 1538.0 example/sec; 0.083 sec/batch)\n",
      "step 2210, loss = 0.95 ( 1683.9 example/sec; 0.076 sec/batch)\n",
      "step 2220, loss = 1.21 ( 1638.7 example/sec; 0.078 sec/batch)\n",
      "step 2230, loss = 0.99 ( 1768.2 example/sec; 0.072 sec/batch)\n",
      "step 2240, loss = 1.12 ( 1769.8 example/sec; 0.072 sec/batch)\n",
      "step 2250, loss = 1.16 ( 1992.4 example/sec; 0.064 sec/batch)\n",
      "step 2260, loss = 1.07 ( 1708.2 example/sec; 0.075 sec/batch)\n",
      "step 2270, loss = 1.19 ( 1635.1 example/sec; 0.078 sec/batch)\n",
      "step 2280, loss = 1.02 ( 1702.5 example/sec; 0.075 sec/batch)\n",
      "step 2290, loss = 1.11 ( 1703.8 example/sec; 0.075 sec/batch)\n",
      "step 2300, loss = 1.24 ( 1795.5 example/sec; 0.071 sec/batch)\n",
      "step 2310, loss = 1.11 ( 1739.2 example/sec; 0.074 sec/batch)\n",
      "step 2320, loss = 1.07 ( 1850.3 example/sec; 0.069 sec/batch)\n",
      "step 2330, loss = 1.38 ( 1633.6 example/sec; 0.078 sec/batch)\n",
      "step 2340, loss = 0.86 ( 1621.2 example/sec; 0.079 sec/batch)\n",
      "step 2350, loss = 1.07 ( 1623.5 example/sec; 0.079 sec/batch)\n",
      "step 2360, loss = 1.03 ( 1944.4 example/sec; 0.066 sec/batch)\n",
      "step 2370, loss = 1.10 ( 1578.0 example/sec; 0.081 sec/batch)\n",
      "step 2380, loss = 1.02 ( 1645.6 example/sec; 0.078 sec/batch)\n",
      "step 2390, loss = 1.19 ( 1713.7 example/sec; 0.075 sec/batch)\n",
      "step 2400, loss = 1.26 ( 1742.4 example/sec; 0.073 sec/batch)\n",
      "step 2410, loss = 1.09 ( 1718.5 example/sec; 0.074 sec/batch)\n",
      "step 2420, loss = 1.09 ( 1529.5 example/sec; 0.084 sec/batch)\n",
      "step 2430, loss = 1.13 ( 2186.8 example/sec; 0.059 sec/batch)\n",
      "step 2440, loss = 1.17 ( 1543.0 example/sec; 0.083 sec/batch)\n",
      "step 2450, loss = 1.09 ( 1738.1 example/sec; 0.074 sec/batch)\n",
      "step 2460, loss = 1.05 ( 1677.7 example/sec; 0.076 sec/batch)\n",
      "step 2470, loss = 1.00 ( 1787.6 example/sec; 0.072 sec/batch)\n",
      "step 2480, loss = 1.24 ( 1692.3 example/sec; 0.076 sec/batch)\n",
      "step 2490, loss = 1.07 ( 1878.9 example/sec; 0.068 sec/batch)\n",
      "step 2500, loss = 0.92 ( 1814.3 example/sec; 0.071 sec/batch)\n",
      "step 2510, loss = 1.05 ( 1740.4 example/sec; 0.074 sec/batch)\n",
      "step 2520, loss = 1.17 ( 1587.0 example/sec; 0.081 sec/batch)\n",
      "step 2530, loss = 1.07 ( 1484.8 example/sec; 0.086 sec/batch)\n",
      "step 2540, loss = 1.09 ( 1655.2 example/sec; 0.077 sec/batch)\n",
      "step 2550, loss = 1.02 ( 1617.5 example/sec; 0.079 sec/batch)\n",
      "step 2560, loss = 1.21 ( 1742.1 example/sec; 0.073 sec/batch)\n",
      "step 2570, loss = 1.11 ( 1606.0 example/sec; 0.080 sec/batch)\n",
      "step 2580, loss = 0.96 ( 1713.9 example/sec; 0.075 sec/batch)\n",
      "step 2590, loss = 0.96 ( 1570.2 example/sec; 0.082 sec/batch)\n",
      "step 2600, loss = 0.91 ( 1747.3 example/sec; 0.073 sec/batch)\n",
      "step 2610, loss = 1.18 ( 1899.8 example/sec; 0.067 sec/batch)\n",
      "step 2620, loss = 0.96 ( 1677.2 example/sec; 0.076 sec/batch)\n",
      "step 2630, loss = 1.26 ( 1956.5 example/sec; 0.065 sec/batch)\n",
      "step 2640, loss = 1.19 ( 1513.2 example/sec; 0.085 sec/batch)\n",
      "step 2650, loss = 1.12 ( 1507.6 example/sec; 0.085 sec/batch)\n",
      "step 2660, loss = 1.09 ( 1670.0 example/sec; 0.077 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2670, loss = 0.88 ( 1690.0 example/sec; 0.076 sec/batch)\n",
      "step 2680, loss = 0.95 ( 1941.6 example/sec; 0.066 sec/batch)\n",
      "step 2690, loss = 1.19 ( 1830.7 example/sec; 0.070 sec/batch)\n",
      "step 2700, loss = 1.02 ( 1757.7 example/sec; 0.073 sec/batch)\n",
      "step 2710, loss = 1.01 ( 1867.0 example/sec; 0.069 sec/batch)\n",
      "step 2720, loss = 1.09 ( 1858.6 example/sec; 0.069 sec/batch)\n",
      "step 2730, loss = 0.98 ( 1838.2 example/sec; 0.070 sec/batch)\n",
      "step 2740, loss = 0.97 ( 1940.0 example/sec; 0.066 sec/batch)\n",
      "step 2750, loss = 1.14 ( 1724.8 example/sec; 0.074 sec/batch)\n",
      "step 2760, loss = 1.12 ( 1965.1 example/sec; 0.065 sec/batch)\n",
      "step 2770, loss = 1.19 ( 1436.1 example/sec; 0.089 sec/batch)\n",
      "step 2780, loss = 1.03 ( 1803.6 example/sec; 0.071 sec/batch)\n",
      "step 2790, loss = 1.09 ( 1663.2 example/sec; 0.077 sec/batch)\n",
      "step 2800, loss = 1.17 ( 1786.5 example/sec; 0.072 sec/batch)\n",
      "step 2810, loss = 1.10 ( 1537.7 example/sec; 0.083 sec/batch)\n",
      "step 2820, loss = 1.25 ( 1740.6 example/sec; 0.074 sec/batch)\n",
      "step 2830, loss = 1.17 ( 1665.7 example/sec; 0.077 sec/batch)\n",
      "step 2840, loss = 0.98 ( 1453.9 example/sec; 0.088 sec/batch)\n",
      "step 2850, loss = 0.92 ( 1807.3 example/sec; 0.071 sec/batch)\n",
      "step 2860, loss = 1.05 ( 1651.4 example/sec; 0.078 sec/batch)\n",
      "step 2870, loss = 1.11 ( 1729.7 example/sec; 0.074 sec/batch)\n",
      "step 2880, loss = 1.06 ( 1554.8 example/sec; 0.082 sec/batch)\n",
      "step 2890, loss = 0.93 ( 1991.3 example/sec; 0.064 sec/batch)\n",
      "step 2900, loss = 1.09 ( 1598.0 example/sec; 0.080 sec/batch)\n",
      "step 2910, loss = 1.07 ( 1734.8 example/sec; 0.074 sec/batch)\n",
      "step 2920, loss = 1.04 ( 1698.2 example/sec; 0.075 sec/batch)\n",
      "step 2930, loss = 1.16 ( 1672.3 example/sec; 0.077 sec/batch)\n",
      "step 2940, loss = 1.10 ( 1692.6 example/sec; 0.076 sec/batch)\n",
      "step 2950, loss = 1.11 ( 1730.1 example/sec; 0.074 sec/batch)\n",
      "step 2960, loss = 1.12 ( 1805.1 example/sec; 0.071 sec/batch)\n",
      "step 2970, loss = 1.00 ( 1647.4 example/sec; 0.078 sec/batch)\n",
      "step 2980, loss = 1.16 ( 1852.8 example/sec; 0.069 sec/batch)\n",
      "step 2990, loss = 0.91 ( 1605.9 example/sec; 0.080 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "for step in (range(max_steps)):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict = {image_holder:image_batch, label_holder:label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 == 0:\n",
    "        example_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        format_str = ('step %d, loss = %.2f ( %.1f example/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step,loss_value, example_per_sec,sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples / batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch , label_batch = sess.run([images_test, labels_test])\n",
    "    predictions = sess.run([top_k_op],feed_dict = {image_holder:image_batch,label_holder2:label_batch})\n",
    "#     test_acc = accuracy.eval(feed_dict = {image_holder:image_batch,label_holder:label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = true_count / total_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7093552215189873"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
